\documentclass[11pt, a4paper, leqno]{article}
\usepackage{a4wide}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float, afterpage, rotating, graphicx}
\usepackage{epstopdf}
\usepackage{longtable, booktabs, tabularx}
\usepackage{fancyvrb, moreverb, relsize}
\usepackage{eurosym, calc}
% \usepackage{chngcntr}
\usepackage{amsmath, amssymb, amsfonts, amsthm, bm}
\usepackage{caption}
\usepackage{mdwlist}
\usepackage{xfrac}
\usepackage{setspace}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\usepackage{minibox}
% \usepackage{endfloat} % Enable to move tables / figures to the end. Useful for some
% submissions.
\usepackage[backend=bibtex, style=authoryear, natbib]{biblatex}

\addbibresource{refs.bib}

\usepackage[unicode=true]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    anchorcolor=black,
    citecolor=NavyBlue,
    filecolor=black,
    menucolor=black,
    runcolor=black,
    urlcolor=NavyBlue
}


\widowpenalty=10000
\clubpenalty=10000

\setlength{\parskip}{1ex}
\setlength{\parindent}{0ex}
\setstretch{1.5}


\begin{document}

\title{Research Proposal: Specialization Trends in Economics Research using JEL Codes \\
Text Data in Economics by Aapo Stenhammar \\
University of Bonn, Winter Term 2024/25}

\author{Felix Schmitz\footnotemark[1]}

\date{
    \today
}

\maketitle
\footnotetext[1]{Email: \href{mailto:s87fschm@uni-bonn.de}{s87fschm@uni-bonn.de}, Matriculation Number: 50173514, For replication material see \href{https://github.com/felixschmitz/econ_spec_jel}{here}}

\section{Introduction}
This paper proposes research to investigate specialization trends within economics research, focusing on whether subfields are becoming more specialized over time.
Understanding these trends is crucial as increasing specialization can influence knowledge dissemination, collaboration patterns, and the evolution of economic thought.

To analyze specialization, a robust measure is required.
The focus will be on leveraging Journal of Economic Literature (JEL) Codes, which are widely used by economists to classify research.
These codes provide a structured way to categorize papers into subfields, allowing for a systematic analysis of specialization trends.
The JEL Codes are assigned by authors or editors and are intended to reflect the main topics of a paper.
While they do not cover the methods or data used in a paper, they provide a useful starting point for understanding the content of a paper, and hence of economic research in aggregate.
By examining the intersections of JEL Codes across papers, this study assesses the depth and diversity of specialization within economics research.

This research also aims to validate the relevance of JEL Codes as a tool for analyzing specialization trends.
Additionally, as an empirical application, the goal is to explore specialization dynamics within the subfield of Labor Economics, providing a focused example of how specialization may evolve over time.
Through this approach, the study contributes to a better understanding of how economic research is structured and how subfields are connected or fragmented (e.g. \citep{davis2019}).
The findings may have implications for how economists engage in interdisciplinary work and how academic communities organize knowledge.

The following sections of this research proposal are structured as follows.
First, the literature review provides a brief overview of the existing research on specialization trends in economics and the use of JEL Codes.
Secondly, the data used for the analysis is described, including the data collection process and pre-processing steps.
Descriptive statistics are then presented to provide an overview of the data.
Next, the methodology is outlined, focusing on the use of topic modeling and network analysis to identify specialization trends.
Finally, the conclusion summarizes the research objectives and the potential implications of the study.
It also outlines the next steps for the analysis and the expected contributions of the research.

\section{Literature Review}
The literature on specialization trends in economics research is limited, with few studies focusing on the evolution of subfields within the discipline.
\citet{onder2021} analyze the specialization trends of authors and co-authors in economics research, and the resulting impact on research output.
They find that eventhough specialization of authors is increasing, the publication by them into highly ranked journals is decreasing.
Recent advances in text data analysis have opened up new possibilities for studying specialization trends \citep{anauati2016, angrist2017, angrist2020, galiani2023a}.
These facilitate natural language processing techniques to assign specific field of economics research tags.
The analysis then is based upon these tags to identify specialization trends on the distinction of theoretical vs. empirical developments.
The tags not only contain information on the topics covered, but also the approach and methods used in the research.
\citet{heikkila2022a} investigates the use of JEL codes in economics research, and their development over time.
The study finds that JEL codes are widely used to classify research in economics, and highlights the potential for "bibliometric and scientometric research" using these codes.
In addition, the research shows that classification scheme of JEL Codes remained basically unchanged since a large revision in 1991.
There is further research on the classification of research papers in economics between human and machine classification \citep{heikkila2024}.
Generating (artificial) JEL Codes from abstracts using large language models is the focus here.
There is relevant research on the trends of JEL Codes used in economics research \citep{kelly2011}.
The authors analyze the publication by subfields, the development of JEL Codes, and define specialty journals using JEL codes.
\citet{rath2016} extend this research focusing on the distribution of JEL Codes in economics research per subfield.
Further, there is a growing interest in understanding how knowledge is structured and how research communities are organized.

This research paper aims to contribute to the existing literature by validating the use of JEL Codes as a tool for analyzing specialization trends in economics research.
In addition, the study will add a different perspective through the use of JEL Codes instead of predicted fields of economics research tags, and allow for a perspective on how authors themselves classify their research.
To be more precise, the self-classification of JEL Codes and the subsequent dynamics might differ from the classification based upon text data based methods.

\section{Data}
For the sake of simplicity, the analysis of this paper are based on the IZA Discussion Paper Series.
The working paper series is well established in the field of Labor Economics and was easily accessible via scraping.
The series has been initialized in April 1998 and has been continuously published since then.
The data includes the JEL Codes, the name of the authors, the abstract and the publication date of the discussion paper, among others.

For a future project, it would be interesting to extend the analysis to other working paper series or general economics journals.
A dataset that might be easily available for analysis is a combination of Constellate, Semantic Scholar, and manually classified publications into subfields of economics research, as produced by \cite{galiani2023a}.
This dataset would provide a more comprehensive view of specialization trends in economics research, and would allow to join the work with .

\subsection{Data Collection}
The data was scraped from the IZA website using the Python library \texttt{requests} and modules from the 'html-parser-library' \texttt{bs4.BeautifulSoup}.
For each discussion paper, IZA has a webpage with metadata and a link to the discussion paper file.
Both the metadata and the file were downloaded and stored locally.
The metadata for each discussion paper contains a title, a list of authors, the publication date (year and month), the list of assigned JEL Codes, a list of assigned keywords, and an abstract, among others.
It was also possible to extract links provided for individual authors, which, if they exist, reference an IZA profile page with further information on the author, or a personal website.
The files and keywords were not used for the analysis so far.
For this analysis, data has been collected up to February 2025 (discussion paper number 17695).

There are 6 digits in the current range without a corresponding discussion paper accessible.
The numbers are (4953, 12119, 12315, 12577, 12664, 16580).
The first discussion paper would have been from May 2010.
The later five ones fall into the time span January 2019 to November 2023.
It is not possible to tell why and when these discussion papers have been removed by IZA.

\subsection{Data Pre-Processing}
Initially, the metadata of the discussion papers was merged into a single dataset.
This dataset was then cleaned and pre-processed to ensure consistency and quality for the analysis.

There are 9 discussion paper entries without a corresponding discussion paper file accessible.
The numbers are (9, 18, 21, 28, 33, 56, 173, 16801, 17143).
The first nine fall into the range June 1998 to July 2000.
The later two are from February and July 2024.
These 15 entries (missing discussion paper or missing file) are excluded from the analysis.

Variables were cleaned and efficient data types were chosen.
Text information regarding publication was extracted from the metadata where possible.
This information was then categorized into 'published', 'forthcoming', 'other\_publication\_information', 'superseded'.
This categorization used text methods on Levenshtein distance, fuzzy search, and string matching, to identify typos in the publication information.
The raw publication information was retained in a separate column.
Information on superseded discussion papers was used, to exclude these from the analysis.
This allowed to exclude discussion papers, which otherwise would have been counted twice in the analysis.
The combination of text methods to identify superseded discussion papers was combined with the publication date of the discussion paper for this step.
This led to dropping 18 entries.

The JEL Codes were cleaned with high precision.
Deviations from the expected format of alphanumeric characters, beginning with a capital letter and followed by two digits, were manually corrected.
There were about 45 discussion papers that needed manual attention to fix the extracted JEL Codes.
In addition, the following algorithmic methods were used to clean the JEL Codes.
The letters were converted to uppercase, leading '1' and '0' were replaced by 'I' and 'O', respectively.
This confusion is likely due to the extraction of the JEL Codes via Optical Character Recognition (OCR) from the PDFs.
After these thoroughly cleaning steps, JEL Codes not starting with a capital letter followed followed by at least one and at most two digits were dropped.
Finally, JEL Codes with a single digit were converted to two digits by adding a trailing zero.
Overall, one discussion paper each was dropped with the JEL Codes "workfromhomecommutetimesallocationoftimesavingsCOVID19" (15870), and
"labordisplacementenergytransitioncoalmines" (15581).
Further, 16 discussion papers were dropped due to completely missing JEL Codes. These were from March 2024 to February 2025.

In a last step, the dataset was limited to discussion papers with an inclusive publication date between January 2000 and December 2024.
This is mainly to ensure a sufficient number of observations per month (for the early years) and complete information on the discussion paper (for the most recent discussion papers).
This reduces the sample size by 195 observations. The final number of discussion papers analysed is 17,449.

\subsection{Descriptive Statistics}
To get an overview of the data, the following descriptive statistics are presented.
Statistics of interest are related to discussion papers, JEL Codes, and authors.
In Figure \ref{fig:dp_counts}, it can be seen that the number of discussion papers published per month has increased over time from less than 20 to over 80 per month.
Interesting to observe is the positive impact of the initial period of the COVID-19 pandemic on the number of discussion papers published.
The average number of JEL Codes per discussion paper on the other hand is rather stable over time, and fluctuates only mildly.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../bld/figures/fig_dp_counts}
    \caption{Number of Discussion Papers \& JEL Codes per Discussion Paper}
    \label{fig:dp_counts}
\end{figure}

\section{Methodology}
The methodologies presented here are used to achieve two main objectives.
First, the behavior of subfields over time is to be analyzed.
Second, the text data methods, such as topic modeling using Latent Dirichlet Allocation (LDA), are used to validate the JEL Codes as a tool for analyzing specialization trends.

\subsection{Topic Modeling}
To analyze the specialization trends in economics research, topic modeling is used.
Specifically, the LDA algorithm is used to identify topics in the abstracts of the discussion papers \cite{blei2003}.
To prepare the abstract data for the LDA model, the text is lowercased, tokenized, and stemmed.
Numbers, punctuation, and words containing no alphanumeric characters are removed.
Furthermore, stopwords are removed from the text.
The model
The number of topics is set to 750 and 20 passes through the corpus of tokenized and stemmed abstracts are performed during training of the model, similar to \cite{galiani2023a}.

\subsection{Topic Modeling}

To analyze specialization trends in economics research, topic modeling is employed to identify latent topics within the abstracts of discussion papers.
Specifically, the Latent Dirichlet Allocation (LDA) generative model is used, which assumes that documents discussing similar topics tend to use similar groups of words \cite{blei2003}.
LDA is particularly suitable for this analysis as it captures co-occurring word clusters, revealing thematic structures within the corpus.
This approach models documents as random mixtures over latent topics, where each topic is characterized by a distribution over words.

To prepare the abstract data for the LDA model, several preprocessing steps are conducted to enhance the quality of the input text.
First, all text is converted to lowercase.
Tokenization is then performed, splitting the text into individual words.
Stemming is applied to reduce words to their root form, capturing the core meaning of words (e.g., replacing "running" with "run").
Additionally, numbers, punctuation, and tokens without alphanumeric characters are removed.
Stopwords, which are common function words with little semantic value, are also eliminated.
These preprocessing steps collectively improve the quality of the tokenized text, ensuring a more effective training process for the LDA model.

The LDA model is trained using \texttt{Gensim}'s implementation of the LDA training algorithm
%\cite{rehurek2010}.
The number of topics is set to 750, and the model is trained using 20 passes through the corpus.
These settings are chosen to ensure sufficient topic granularity while maintaining interpretability, similar to the approach used in \cite{galiani2023a}.

% is this true?
The model uses a symmetric Dirichlet prior for the topic distribution per document ($\alpha$) and the word distribution per topic ($\eta$), reflecting the assumption that all topics and words are equally likely a priori.

Two main outputs are obtained from the trained LDA model. First, the word distribution over topics, denoted as $w_{k,l}$, represents the probability of word $l$ in topic $k$, with $\sum_l w_{k,l} = 1$.
This indicates the contribution of each word to the topic.
Second, the topic distribution over documents, denoted as $t_{i,k}$, captures the presence of topic $k$ in document $i$, with $\sum_k t_{i,k} = 1$.
This shows the relevance of each topic within the document.

To ensure the meaningfulness of the detected topics, a filtering process is applied.
Topics predominantly composed of non-thematic words (e.g., "online", "access", "appendix") or those lacking a clear thematic pattern are excluded.
% After this filtering process, 695 meaningful topics are retained.
An article $i$ is defined as "containing" topic $k$ if $t_{i,k}$ ranks among the top ten topics with the highest presence in that article.
This approach ensures consistent topic assignment while maintaining interpretability.

This topic modeling approach not only identifies prevalent themes but also captures the interconnectedness of topics within the field.
It provides a robust foundation for analyzing specialization trends in economics research by revealing how topics cluster and evolve over time.
The results from this robust methodology offer a comprehensive benchmark for understanding the dynamics of specialization within the field when using JEL codes.

In Figure \ref{fig:top5_increasing_decreasing_topics}, the top 5 topics increasing and decreasing in presence are shown.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../bld/figures/fig_topic_trends}
    \caption{Top 5 Topics Increasing (Left) and Decreasing (Right) in Presence}
    \label{fig:top5_increasing_decreasing_topics}
\end{figure}

\subsection{JEL Code Analysis}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../bld/figures/fig_top5overall_jel}
    \caption{Top 5 JEL Codes}
    \label{fig:top5overall_jel}
\end{figure}

\subsection{Overlapping Network Analysis}
Initially, I wanted to use the Louvain algorithm to detect communities in the network.
However, the algorithm only allows to non-overlapping communities, that is, a node can only belong to one community.
This is not suitable for the analysis of specialization trends in economics research, as a JEL code can belong to multiple subfields.
Rather the combination of JEL codes describes a subfield.
Subsets of JEL codes describing a subfield might overlap with JEL codes describing another subfields.
Hence, the goal is to use a different algorithm to detect overlapping communities in the network.

\section{Conclusion}
In a next step, the self-assigned keywords of the discussion papers can be used for comparison with the results from the topic modeling and JEL Code analysis.
Further, the content of the files can be used to create artificial JEL Codes as in \citet{heikkila2024} or to create fields of economics research tags as shown by \citet{anauati2016, galiani2023a}.
Leveraging the data from \citet{anauati2016, galiani2023a} would also allow to expand the analysis into other subfields of economics research, and to combine the JEL Code analysis with citation data.
This would allow to understand how specialization trends in economics research are related to citation patterns and the dissemination of knowledge for specific topics.

\printbibliography

\clearpage

\section{Appendix}

There are few authors that have published a large number of discussion papers.
Similarly, there are few JEL Codes that are assigned to a large number of discussion papers.
The Figure \ref{fig:counts_dp_jel_codes} shows the distribution of discussion papers by author on the left and by JEL Codes on the right.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../bld/figures/fig_counts_dp_jel_codes}
    \caption{Discussion Papers by Author \& Discussion Papers by JEL Codes}
    \label{fig:counts_dp_jel_codes}
\end{figure}

Relevant to note is, that the structure of the authors publishing discussion papers has changed over time.
The average number of authors per discussion paper per month has increased by about 0.5 authors per discussion paper.
Furthermore, the average new number of new authors per discussion paper per month is stable around 0.5 authors per discussion paper, while the average number of authors that have published before is increasing by about 1.5 authors per discussion paper per month.
This fact can be related to the establishment of the discussion series as a well-known platform for research in Labor Economics.
These trends are shown in Figure \ref{fig:author_trends}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../bld/figures/fig_author_trends}
    \caption{Average Number of Authors Over Time}
    \label{fig:author_trends}
\end{figure}

In Figure \ref{fig:top3yearl_jel}, all JEL Codes are shown that are at least in one year among the top 3 JEL Codes.
If the JEL Code is not among the top 3 JEL Codes for a specific year in the plot, the opacity of the line is reduced.
The JEL Codes 'J24' (Human Capital, Skills, Occupational Choice, Labor Productivity) and 'J31' (Wage Level and Structure, Wage Differentials) are consistently among the top 3 JEL Codes.
While other JEL Codes like 'J16' (Economics of Gender, Non-labor Discrimination) saw a relative frequency increase lately.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../bld/figures/fig_top3yearly_jel}
    \caption{Top 3 JEL Codes yearly}
    \label{fig:top3yearly_jel}
\end{figure}

\end{document}
